---
title: 'AI will create a near-term socioeconomic gap among knowledge workers'
publishedAt: 2025-07-06
summary: 'The primary beneficiaries of AI are knowledge workers with existing expertise, which will create an increasingly high barrier of entry for those jobs.'
categories: ['Nerdery', 'AI']
---

Despite the misleading hype cycle[^1], AI is fundamentally changing how people perform their work.

Across the large engineering team I work with, there is a spectrum of viewpoints about using AI, ranging (roughly) from “let’s try handing it the keys” to “I’m inherently skeptical of non-deterministic changes to our codebase.” 

No matter the level of skepticism, though, no engineer I’ve talked to wants to go back to a world where AI isn’t at their disposal. One of our best engineers, who is on the more skeptical end of the spectrum, told me recently, “many people are thinking about and using AI the wrong way, but if you learn how to use the tool correctly, it can seem like magic.” I agree. 

On a team level, I’ve seen material productivity gains. Almost a year ago, I was tasked with rebuilding a function and initially reduced the headcount size by 60%, planning to rapidly rehire. Instead of backfilling, though, I kept the team small and velocity increased. A primary driver of that outcome was having the right people in place and motivating them, but using AI to research, rapidly prototype and support our processes dramatically increased the output of everyone on the team, acting as a measurable force-multiplier on existing talent. 

Personally, I use multiple AI tools as a core part of my job, side projects[^2] and personal life. Some workflows are complex, especially those that are agentic in nature, spanning multiple tools to accomplish some task. 

AI has become an integral part of how I interact with and use technology, but more importantly, it is reshaping aspects of how I work and even the way I think about how to produce work. 

## Flyover country 

My friend is a product leader and we often talk about how we use AI. In a recent discussion, he made the following statement in passing: 

> AI is normal for us, but there are a huge number of people who haven’t used it, don’t know what it is, or haven’t even heard of it. 

The history AI has already made[^3] at blinding speed makes it easy to forget that outside of Silicon Valley, most of the world is continuing just like it was before. 

Since the conversation, I’ve stepped back to more intentionally observe how other people in my life interact with AI. My goal has been understanding the behavior of people outside the heat of Silicon Valley—who don’t spend their days figuring out how to better use AI, how it is shaping the broader market landscape and how it can shape product experiences in software (my friend Barry McCardle[^4] has called this paying attention to flyover country). 

A one-word summary of my anecdotal research would be **contrast**. 

One of the primary differences I’ve noticed is *creation* versus *optimization*. Me and my team have invented new ways of doing our work, which includes many things that weren’t practically possible before. The people around me who don’t work in tech use AI to do things they were already doing, but faster, more comprehensively, more conveniently.

A few examples: 

**My brother, an avid mountain biker, used AI to help him tune the suspension on his race bike**. It worked incredibly well and was ten times faster than his previous methodology, which included reading owner manuals, searching online for advanced guides and multiple rounds of testing and learning. 

**My uncle, a writer, uses ChatGPT to help him write**. He asks AI to edit his work and help him find just the right word or phrase. 

**My wife has used AI within existing platforms and workflows**. Canva, Squarespace and a multitude of other tools have added AI functionality that makes using their tools faster and in some cases more powerful (though in my experience many of those features feel half-baked). 

## Local maximums and asymmetric value

The contrast makes the difference seem less like two points on a spectrum and more like fundamentally understandings of the technology. 

These people are getting material leverage from AI, but in a way that feels strikingly similar to the problems Amazon Alexa and other voice assistants face: wide adoption of a short list of simple use cases that make existing activities more convenient[^5]—in other words, dramatic underutilization of the technology relative to its capability. 

One core reason for these local maximums is the interface itself. The power is obfuscated by an inanimate physical speaker or blank chat input. People can ask AI anything, but without an interface to help them understand what is possible, they will default to the familiar. This problem will likely be solved over time[^6], but I’m not alone in thinking that the chat interface is one of the great tragedies of early AI[^7]. 

The other core reason is that wielding AI in a way that creates true asymmetric leverage for your work requires deep pre-existing knowledge of and skill in what you are working on. 

If you don’t have foundational knowledge, the gains will always have a logarithmic limit because AI is non-deterministic. You can make a certain amount of progress, but at some point, you don’t know enough to understand and articulate progress in the right direction, so you hit a local maximum—even though LLMs can help you learn about new subjects[^8]. (This is why, despite a world of possibilities with open-ended AI, closed systems will be the big winners in the near-term[^9].)

Almost twenty years ago when I first entered the market as a knowledge worker, I noticed a similar limitation watching people use Google search. Google was a massive leap forward in information retrieval, but a vast majority of users executed search in basic ways that used a fraction of the tool’s potential. 

I clearly remember that a common characteristic of abnormally productive peers was being “really good at Googling.” Technical personas over-indexed for this skill, which isn’t surprising, because those who generated asymmetric value from the search were almost always the ones who had a deeper understanding of how the tool itself worked, both in terms of advanced user features and, more importantly, the search algorithm itself. 

## The rich will get richer 

A good friend of mine used to work in product at Linear. I was blown away when I heard early on how many companies were wiring LLMs into the product in order to automate all kinds of tasks, including writing code for features. 

The statistics shared through the Neon acquisition are even more astounding: 80% of databases were created by AI agents, not humans[^10]. 

Vercel’s CEO estimates that v0 has contributed to quadrupling application creation on their platform[^11]. 

The list goes on, but the punchline is the same: those who are realizing asymmetric value from AI are knowledge workers with the foundational expertise to push beyond the local maximum. 

This is the most logical initial frontier in terms of market economics. If you can get the same or more output from half the knowledge worker headcount, which is one of the most expensive resources, the entire cost structure of a business can change. As Klarna knows[^12], it’s not as simple as flipping a switch, but the change is happening, I believe on a wider scale and faster than it feels like to many people. 

This boon in knowledge worker productivity will be good for business and ultimately the consumer if more efficient resource allocation results in better products and lower prices, but it will also significantly raise the barrier of entry for those jobs.

## The gap

As a team leader at a tech company, there’s no question that hiring is going to become increasingly difficult for me and my peers. Closing and retaining (or training) good talent with the ability to effectively leverage deep foundational knowledge was already hard, but the required skillsets to operate on a lean, hyper-productive team using AI are becoming increasingly specialized. 

The rate of change makes it hard to predict the future, but in the current state, I see the increasing demand for specialized knowledge work talent far outstripping the ability of the market to provide it and there’s no clear pathway for people to people to overcome the gap. This is happening now, but as adoption of advanced AI workflows becomes widespread and mandatory, it will feel to many like a socioeconomic wall materialized suddenly. 

This will be temporary—humanity has survived worse—but I’m buckling up for a bumpy ride over the next few years. 

[^1]: Dimitri Dimandt wrote a [concise, helpful post](https://dmitriid.com/everything-around-llms-is-still-magical-and-wishful-thinking) about AI hype feeling similar to crypto and why it’s hard to make comparisons between people’s experiences. 

[^2]: I recently [used Cursor to migrate this 13-year-old WordPress blog to Next.js and Vercel](/blog/using-cursor-to-migrate-my-wordpress-blog-to-nextjs-and-vercel). 

[^3]: ChatGPT [reached 100 million users](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/) 2 months after launch. Their [revenue almost doubled](https://www.reuters.com/business/media-telecom/openais-annualized-revenue-hits-10-billion-up-55-billion-december-2024-2025-06-09/) from $5.5 billion to $10 billion in the first 6 months of 2025. Cursor [set a record](https://x.com/levie/status/1879409075643183320) for the fastest time to $100M in ARR. 

[^4]: I really appreciate conversations with [Barry](https://www.linkedin.com/in/barrymccardel) because he's thought through so many critical questions on a deep level and articulates them clearly. 

[^5]: [Research shows](https://www.nngroup.com/articles/intelligent-assistants-poor-usability-high-adoption/) that Alexa, Siri and Google Home have all struggled to help users break past basic use cases like searching the internet, checking the weather, getting directions and playing music. 

[^6]: The blank screen problem is a big issue for AI, but I believe it will likely be solved over time. AI is delivered as software, which doesn’t face the same user experience limitations of hardware like smart speakers. Also, the amount of money and mindshare being poured into the space is eye watering. Maybe I’m overestimating the major players, though—the big money is in the API business, so perhaps they will continue to deliver  consumer apps with a primitive interface. 

[^7]: Earlier this year I attended the Data Council conference, where Naveen Rao, VP of AI at Databricks, [made a public appeal](https://www.datacouncil.ai/talks25/keynote-data-meets-intelligence-where-the-data-infra-ai-stack-converge?hsLang=en) that we move past the chat interface. 

[^8]: I sent this post to a few peers for review, and one of the first comments was that LLMs are different than Google in that they are a much better way to learn about new subjects and create foundational knowledge. That’s true in theory, but the learning path is still non-deterministic and as a learner you don’t have heuristics to understand whether you are learning the right things. That’s fine for some subjects and projects, but I’ve seen it cause real problems when producing knowledge work for production. For the record, I’m extremely bullish on the ways that LLMs can improve education, but based on the current state, I think it will take some radical innovation to see those  breakthroughs. 

[^9]: AI requires a significant amount of quality context in order to work well. It’s extremely hard to give AI that context manually, but closed systems like Notion are building platforms where all of the parts are connected. It’s a brilliant strategy and if executed well, it won’t feel like AI, it will feel like abnormally awesome software (sadly, their marketing is following the AI hype cycle). Grammarly seems to be pursuing a similar play with their [acquisition of Superhuman](https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/). This is the same cycle of bundling we’ve seen historically, but significantly accelerated because of how much more powerful AI is in a high-context, bundled environment.

[^10]: The [database creation statistic Databricks published](https://www.databricks.com/company/newsroom/press-releases/databricks-agrees-acquire-neon-help-developers-deliver-ai-systems) as part of the Neon acquisition announcement was fascinating. A major source of database creation [seems to be Replit, v0 and similar tools](https://davidgomes.com/one-year-at-a-database-startup-called-neon/)—to the point that Neon launched their own AI app builder. I use Vercel’s v0 extensively, but I think there’s still a gigantic gap between building a hobby project with an AI tool and shipping production software. 

[^11]: V0 seems to have significant adoption and coupled with Vercel’s incredible deployment platform, it’s not a surprise that they are seeing [massive growth in app creation](https://www.acquired.fm/episodes/building-web-apps-with-just-english-and-ai-with-vercel-ceo-guillermo-rauch). 

[^12]: Klarna tried to replace ~40% of their customer service workforce with AI, but significantly underestimated how hard it is to run product AI systems at a high level of reliability and accuracy, so they [walked it back](https://www.fastcompany.com/91332763/going-ai-first-appears-to-be-backfiring-on-klarna-and-duolingo). AI critics point to this as an example of why things won’t work, but that’s a similar error to judging the future of self-driving cars on early problems. I’m not an AI inevitabilist, but I have enough direct experience to know that there will be workforce disruption. 